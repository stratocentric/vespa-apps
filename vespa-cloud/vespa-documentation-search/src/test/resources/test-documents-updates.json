[
  {
    "fields": {
      "path":      { "assign": "/documentation/access-logging.html"},
      "namespace": { "assign": "open"},
      "title":     { "assign": "Access Logging"},
      "content":   { "assign": ".left, .right {padding-right: 30px;}    The Vespa access log format allows the logs to be processed by a number of available tools handling JSON based (log) files. With the ability to add custom key/value pairs to the log from any Searcher, you can easily track the decisions done by container components for given requests.     Vespa Access Log Format  In the Vespa access log each log event is logged as a single JSON object compressed on a single line. This makes it possible to use basic line-based tools such as grep to process the log file, but a JSON aware tool such as jq is obviously preferred.    Access log fields  The log format defines a list of fields that can be logged with every request. In addition to these fields,  custom key/value pairs can be logged via Searcher code. The following fields are pre-defined by Vespa:      Name Type Description Always present    ip string The IP address request came from yes  time number UNIX timestamp with millisecond decimal precision (e.g. 1477828938.123) yes  duration number The duration of the request in seconds with millisecond decimal precision (e.g. 0.123) yes  responsesize number The size of the response in bytes yes  code number The HTTP status code returned yes  method string The HTTP method used (e.g. 'GET') yes  uri string The request URI from path and beyond (e.g. '/search?query=test') yes  version string The HTTP version (e.g. 'HTTP/1.1') yes  agent string The user agent specified in the request yes  host string The host header provided in the request yes  scheme string The scheme of the request yes  port number The IP port number of the interface on which the request was received yes  remoteaddr string The IP address of the remote client if specified in HTTP header no  remoteport string The port used from the remote client if specified in HTTP header  no  peeraddr string Address of immediate client making request if different than remoteaddr no  peerport string Port used by immediate client making request if different than remoteport no  user-principal string The name of the authenticated user (java.security.Principal.getName()) if principal is set no  ssl-principal string The name of the x500 principal if client is authenticated through SSL/TLS no  search object Object holding search specific fields no  search.totalhits number The total number of hits for the query no  search.hits number The hits returned in this specific response no  search.coverage object Object holding query coverage information similar to that returned in result set. no  attributes object Object holding custom key/value pairs logged in searcher. no     NOTE:IP addresses can be both IPv4 addresses in standard dotted format (e.g. 127.0.0.1) or IPv6 addresses in standard form with leading zeros omitted  (e.g. 2222:1111:123:1234:0:0:0:4321).  An example log line will look like this:  {\"ip\":\"152.200.54.243\",\"time\":920880005.023,\"duration\":0.122,\"responsesize\":9875,\"code\":200,\"method\":\"GET\",\"uri\":\"/search?query=test&amp;param=value\",\"version\":\"HTTP/1.1\",\"agent\":\"Mozilla/4.05 [en] (Win95; I)\",\"host\":\"localhost\",\"search\":{\"totalhits\":1234,\"hits\":0,\"coverage\":{\"coverage\": 98, \"documents\": 100, \"degraded\": {\"non-ideal-state\": true}}}}  A pretty print version of the same example:  {   \"ip\": \"152.200.54.243\",   \"time\": 920880005.023,   \"duration\": 0.122,   \"responsesize\": 9875,   \"code\": 200,   \"method\": \"GET\",   \"uri\": \"/search?query=test&amp;param=value\",   \"version\": \"HTTP/1.1\",   \"agent\": \"Mozilla/4.05 [en] (Win95; I)\",   \"host\": \"localhost\",   \"search\": {     \"totalhits\": 1234,     \"hits\": 0,     \"coverage\": {       \"coverage\": 98,       \"documents\": 100,       \"degraded\": {         \"non-ideal-state\": true       }     }   } }  NOTE: The log format is extendable by design such that the order of the fields can be changed and new fields can be added between minor versions. Make sure any programmatic log handling is using a proper JSON processor.    Logging Remote Address/Port  In some cases when a request passes through an intermediate service, this service may add HTTP headers indicating the IP address and port of the real origin client. These values are logged as remoteaddr and remoteport respectively. Vespa will log the contents in any of the following HTTP request headers as remoteaddr: X-Forwarded-For,  Y-RA, YahooRemoteIP or Client-IP. If more than one of  these headers are present, the precedence is in the order listed here, i.e. X-Forwarded-For takes precedence over Y-RA. The contents of  the Y-RP HTTP request header will be logged as remoteport.  If the remote address or -port differs from those initiating the HTTP request, the address and port for the immediate client making the request are logged as peeraddress and peerport respectively.   Configuring Logging  For details on the access logging configuration see  accesslog in the container element in services.xml   File name pattern  The file name pattern is expanded using the time when the file is created. The following parts in the file name are expanded:         Field Format Meaning Example    %Y YYYY Year 2003  %m MM Month, numeric 08  %x MMM Month, textual Aug  %d dd Date 25  %H HH Hour 14  %M mm Minute 30  %S ss Seconds 35  %s SSS Milliseconds 123  %Z Z Time zone -0400  %T Long System.currentTimeMillis 1349333576093  %% % Escape percentage %       Log rotation  Apache httpd style log rotation can be configured by setting the rotationScheme. There's two alternatives for the rotationScheme, sequence and date. The rotation time is controlled by setting rotationInterval.    Sequence rotation scheme  The fileNamePattern is used for the active log file name (which in this case will often be a constant string). At rotation, this file is given the name fileNamePattern.N where N is 1 + the largest integer found by extracting the integers from all files ending by .&lt;Integer&gt; in the same directory   &lt;accesslog type='json'            fileNamePattern='logs/vespa/qrs/JsonAccessLog.&lt;container id&amp;gt.%Y%m%d%H%M%S'            rotationScheme='sequence' /&gt;    Date rotation scheme  The fileNamePattern is used for the active log file name here too, but the log files are not renamed at rotation. Instead, you must specify a time-dependent fileNamePattern so that each time a new log file is created, the name is unique. In addition, a symlink is created pointing to the active log file. The name of the symlink is specified using symlinkName.   &lt;accesslog type='json'            fileNamePattern='logs/vespa/qrs/JsonAccessLog.&lt;container id&amp;gt.%Y%m%d%H%M%S'            rotationScheme='date'            symlinkName='JsonAccessLog' /&gt;    Rotation interval  The time of rotation is controlled by setting rotationInterval:  &lt;accesslog type='json'            fileNamePattern='logs/vespa/qrs/JsonAccessLog.&lt;container id&amp;gt.%Y%m%d%H%M%S'            rotationInterval='0 60 ...'            rotationScheme='date'            symlinkName='JsonAccessLog.&lt;container id&amp;gt' /&gt;  The rotationInterval is a list of numbers specifying when to do rotation. Each element represents the number of minutes since midnight. Ending the list with '...' means continuing the arithmetic progression defined by the two last numbers for the rest of the day. E.g. \"0 100 240 480 ...\" is expanded to \"0 100 240 480 720 960 1200\"   Logging Key/Value pairs to the JSON Access Log from Searchers  To add a key/value pair to the access log from a searcher, use  query/result.getContext(true).logValue(key,value)  Such key/value pairs may be added from any thread participating in handling the query without incurring synchronization overhead.  If the same key is logged multiple times, the values written will be included in the log as an array of strings rather than a single string value.  The key/value pairs are added to the attributes object in the log.  An example log line will then look something like this:   {\"ip\":\"152.200.54.243\",\"time\":920880005.023,\"duration\":0.122,\"responsesize\":9875,\"code\":200,\"method\":\"GET\",\"uri\":\"/search?query=test&amp;param=value\",\"version\":\"HTTP/1.1\",\"agent\":\"Mozilla/4.05 [en] (Win95; I)\",\"host\":\"localhost\",\"search\":{\"totalhits\":1234,\"hits\":0},\"attributes\":{\"singlevalue\":\"value1\",\"multivalue\":[\"value2\",\"value3\"]}}  A pretty print version of the same example:  {   \"ip\": \"152.200.54.243\",   \"time\": 920880005.023,   \"duration\": 0.122,   \"responsesize\": 9875,   \"code\": 200,   \"method\": \"GET\",   \"uri\": \"/search?query=test&amp;param=value\",   \"version\": \"HTTP/1.1\",   \"agent\": \"Mozilla/4.05 [en] (Win95; I)\",   \"host\": \"localhost\",   \"search\": {     \"totalhits\": 1234,     \"hits\": 0   },   \"attributes\": {     \"singlevalue\": \"value1\",     \"multivalue\": [       \"value2\",       \"value3\"     ]   } }  "}
    }
  },
  {
    "fields": {
      "path":      { "assign": "/documentation/operations/admin-procedures.html"},
      "namespace": { "assign": "open"},
      "title":     { "assign": "Administrative Procedures"},
      "content":   { "assign": "Install  Refer to the multinode install for a primer on how to set up a cluster. Required architecture is x86_64.     System status    Check logs   Use performance graphs, System Activity Report (sar)     or status pages to track load   Use query tracing   Use feed tracing        Use the cluster controller status page (below)     to track the status of search/storage nodes.     Process PID files  All Vespa processes have a PID file $VESPA_HOME/var/run/{service name}.pid, where {service name} is the Vespa service name, e.g. container or distributor. It is the same name which is used in the administration interface in the config sentinel.     Status pages  Vespa service instances have status pages for debugging and testing. Status pages are subject to change at any time -  take care when automating. Procedure         Find the port:     The status pages runs on ports assigned by Vespa. To find status page ports,     use vespa-model-inspect     to list the services run in the application.  $ vespa-model-inspect services      To find the status page port for a specific node for a specific service,     pick the correct service and run:  $ vespa-model-inspect service [Options] &lt;service-name&gt;         Get the status and metrics:     distributor, storagenode, searchnode and     container-clustercontroller are content services with status pages.     These ports are tagged HTTP. The cluster controller have multiple ports tagged HTTP,     where the port tagged STATE is the one with the status page.     Try connecting to the root at the port, or /state/v1/metrics.     The distributor and storagenode status pages are available at /:    $ vespa-model-inspect service searchnode   searchnode @ myhost.mydomain.com : search   search/search/cluster.search/0   tcp/myhost.mydomain.com:19110 (STATUS ADMIN RTC RPC)   tcp/myhost.mydomain.com:19111 (FS4)   tcp/myhost.mydomain.com:19112 (TEST HACK SRMP)   tcp/myhost.mydomain.com:19113 (ENGINES-PROVIDER RPC)   tcp/myhost.mydomain.com:19114 (HEALTH JSON HTTP)   $ curl http://myhost.mydomain.com:19114/state/v1/metrics   ...   $ vespa-model-inspect service distributor   distributor @ myhost.mydomain.com : content   search/distributor/0   tcp/myhost.mydomain.com:19116 (MESSAGING)   tcp/myhost.mydomain.com:19117 (STATUS RPC)   tcp/myhost.mydomain.com:19118 (STATE STATUS HTTP)   $ curl http://myhost.mydomain.com:19118/state/v1/metrics   ...   $ curl http://myhost.mydomain.com:19118/   ...      A status page for the cluster controller is available at the status port at http://hostname:port/clustercontroller-status/v1/&lt;clustername&gt;. If clustername is not specified, the available clusters will be listed. The cluster controller leader status page will show if any nodes are operating with differing cluster state versions. It will also show how many data buckets are pending merging (document set reconciliation) due to either missing or being out of sync.  $ vespa-model-inspect service container-clustercontroller | grep HTTP  With multiple cluster controllers, look at the one with a \"/0\" suffix in its config ID; it is the preferred leader.  The cluster state version is listed under the SSV table column. Divergence here usually points to host or networking issues.         Cluster state  Cluster and node state information is available through the State Rest API. This API can also be used to set a user state for a node - alternatively use:    vespa-get-cluster-state   vespa-get-node-state   vespa-set-node-state   Also see the cluster controller status page.  Some state is persisted in a ZooKeeper cluster, restarting/changing a cluster controller preserves this:    The last cluster state version is stored, such that a cluster controller       taking over can continue the version numbers where the old one left of   The unit states (set by operators) are stored  In case of state data lost, the cluster state is reset - see cluster controller for implications.     Cluster controller configuration  To configure the cluster controller, use services.xml and/or add  configuration under the services element - example:  &lt;services version=\"1.0\"&gt;     &lt;config name=\"vespa.config.content.fleetcontroller\"&gt;         &lt;min_time_between_new_systemstates&gt;5000&lt;/min_time_between_new_systemstates&gt;     &lt;/config&gt;      Overload  Disk and/or memory might be exhausted and block feeding - recover from feed block  Prioritized partition queues on the content nodes. The metric to look out for is the .filestor.alldisks.queuesize metric, giving a total value for all the partitions. While queue size is the most dependable metric, operations can be queued elsewhere too, which may be able to hide the issue. There are some visitor related queues: .visitor.allthreads.queuesize and .visitor.cv_queuesize.  Many applications built on Vespa is used by multiple clients and faces the problem of protecting clients from abuse. Use the  RateLimitingSearcher to rate limit load from each client type.  Monitor CPU, memory and IO usage. Note: A fully utilized resource does not necessarily indicate overload. As the content cluster supports prioritized operations, it will typically do as many low priority operations as it is able to when no high priority operations are in the queue. This means, that even if there's just a low priority reprocess or a load rebalancing effort going on after a node went down, the cluster may still use up all available resources to process these low priority tasks.  Track network bandwidth consumption and switch saturation.     Monitor distance to ideal state  Refer to the distribution algorithm. Distributor status pages can be viewed to manually inspect state metrics:                .idealstate.idealstate_diff          This metric tries to create a single value indicating distance to the ideal state.         A value of zero indicates that the cluster is in the ideal state.         Graphed values of this metric gives a good indication for how         fast the cluster gets back to the ideal state after changes.         Note that some issues may hide other issues, so sometimes the graph         may appear to stand still or even go a bit up again, as resolving one         issue may have detected one or several others.             .idealstate.buckets_toofewcopies          Specifically lists how many buckets have too few copies.         Compare to the buckets metric to see how big a portion of the cluster this is.             .idealstate.buckets_toomanycopies          Specifically lists how many buckets have too many copies.         Compare to the buckets metric to see how big a portion of the cluster this is.             .idealstate.buckets          The total number of buckets managed. Used by other metrics reporting         bucket counts to know how big a part of the cluster they relate to.             .idealstate.buckets_notrusted          Lists how many buckets have no trusted copies.         Without trusted buckets operations against the bucket may have poor performance,         having to send requests to many copies to try and create consistent replies.             .idealstate.delete_bucket.pending          Lists how many buckets that needs to be deleted.             .idealstate.merge_bucket.pending          Lists how many buckets there are,         where we suspect not all copies store identical document sets.             .idealstate.split_bucket.pending          Lists how many buckets are currently being split.             .idealstate.join_bucket.pending          Lists how many buckets are currently being joined.             .idealstate.set_bucket_state.pending          Lists how many buckets are currently altered for active state.         These are high priority requests which should finish fast,         so these requests should seldom be seen as pending.               Cluster reconfig / restart  Notes:    Running vespa-deploy prepare will not change served     configurations until vespa-deploy activate is run.     vespa-deploy prepare will warn about all config changes that require restart.        Refer to schemas for how to add/change/remove these.        Refer to elastic Vespa     for details on how to add/remove capacity from a Vespa cluster.        See  chained components     for how to add or remove searchers and document processors.       Add or remove services on a node  It is possible to run multiple Vespa services on the same host. If changing the services on a given host, stop Vespa on the given host before running vespa-deploy activate. This is because the services will be allocated port numbers depending on what is running on the host. Consider if some of the services changed are used by services on other hosts. In that case, restart services on those hosts too. Procedure:    Edit services.xml and hosts.xml   Stop Vespa on the nodes that have changes   Run vespa-deploy prepare and vespa-deploy activate   Start Vespa on the nodes that have changes     Add/remove/modify chains  Searcher, Document processing and Processing chains can be modified at runtime without restarts. Modification includes adding/removing processors in chains and changing names of chains and processors. Make the change and deploy. Some changes require a container restart, refer to  reconfiguring document processing.    Configure grouped setup  Refer to the sizing examples for changing from a flat to grouped cluster.    Restart distributor node  When a distributor stops, it will try to respond to any pending cluster state request first. New incoming requests after shutdown is commenced will fail immediately, as the socket is no longer accepting requests. Cluster controllers will thus detect processes stopping almost immediately.  The cluster state will be updated with the new state internally in the cluster controller. Then the cluster controller will wait for maximum  min_time_between_new_systemstates before publishing the new cluster state - this to reduce short-term state fluctuations.  The cluster controller has the option of setting states to make other distributors take over ownership of buckets, or mask the change, making the buckets owned by the distributor restarting unavailable for the time being. Distributors restart fast, so the restarting distributor may transition directly from up to initializing. If it doesn't, current default behavior is to set it down immediately.  If transitioning directly from up to initializing, requests going through the remaining distributors will be unaffected. The requests going through the restarting distributor will immediately fail when it shuts down, being resent automatically by the client. The distributor typically restart within seconds, and syncs up with the service layer nodes to get metadata on buckets it owns, in which case it is ready to serve requests again.  If the distributor transitions from up to down, and then later to initializing, other distributors will request metadata from the service layer node to take over ownership of buckets previously owned by the restarting distributor. Until the distributors have gathered this new metadata from all the service layer nodes, requests for these buckets can not be served, and will fail back to client. When the restarting node comes back up and is marked initializing or up in the cluster state again, the additional nodes will dump knowledge of the extra buckets they previously acquired.  For requests with timeouts of several seconds, the transition should be invisible due to automatic client resending. Requests with a lower timeout might fail, and it is up to the application whether to resend or handle failed requests.  Requests to buckets not owned by the restarting distributor will not be affected. The other distributors will start to do some work though, affecting latency, and distributors will refetch metadata for all buckets they own, not just the additional buckets, which may cause some disturbance.    Restart content node  When a content node restarts in a controlled fashion, it marks itself in the stopping state and rejects new requests. It will process its pending request queue before shutting down. Consequently, client requests are typically unaffected by content node restarts. The currently pending requests will typically be completed. New copies of buckets will be created on other nodes, to store new requests in appropriate redundancy. This happens whether node transitions through down or maintenance state. The difference being that if transitioning through maintenance state, the distributor will not start any effort of synchronizing new copies with existing copies. They will just store the new requests until the maintenance node comes back up.  When coming back up, content nodes will start with gathering information on what buckets it has data stored for. While this is happening, the service layer will expose that it is initializing, but not done with the bucket list stage. During this time, the cluster controller will not mark it initializing in cluster state yet. Once the service layer node knows what buckets it has, it reports that it is calculating metadata for the buckets, at which time the node may become visible as initializing in cluster state. At this time it may start process requests, but as bucket checksums have not been calculated for all buckets yet, there will exist buckets where the distributor doesn't know if they are in sync with other copies or not.  The background load to calculate bucket checksums has low priority, but load received will automatically create metadata for used buckets. With an overloaded cluster, the initializing step may not finish before all buckets have been initialized by requests. With a cluster close to max capacity, initializing may take quite some time.  The cluster is mostly unaffected during restart. During the initializing stage, bucket metadata is unknown. Distributors will assume other copies are more appropriate for serving read requests. If all copies of a bucket are in an initializing state at the same time, read requests may be sent to a bucket copy that does not have the most updated state to process it.     Troubleshooting   Distributor or content node not existing  Content cluster nodes will register in the vespa-slobrok naming service on startup. If the nodes have not been set up or fail to start required processes, the naming service will mark them as unavailable.  Effect on cluster: Calculations for how big percentage of a cluster that is available will include these nodes even if they never have been seen. If many nodes are configured, but not in fact available, the cluster may set itself offline due by concluding too many nodes are down.    Content node not available on the network  vespa-slobrok requires nodes to ping it periodically. If they stop sending pings, they will be set as down and the cluster will restore full availability and redundancy by redistribution load and data to the rest of the nodes. There is a time window where nodes may be unavailable but still not set down by slobrok.  Effect on cluster: Nodes that become unavailable will be set as down after a few seconds. Before that, document operations will fail and will need to be resent. After the node is set down, full availability is restored. Data redundancy will start to restore.    Distributor or content node crashing  A crashing node restarts in much the same node as a controlled restart. A content node will not finish processing the currently pending requests, causing failed requests. Client resending might hide these failures, as the distributor should be able to process the resent request quickly, using other copies than the recently lost one.    Thrashing nodes  An example is OS disk using excessive amount of time to complete IO requests. Eventually the maximum number of files are open, and as the OS is so dependent on the filesystem, it ends up not being able to do much at all.  get-node-state requests from the cluster controller fetch node metrics from /proc and write this to a temp directory on the disk before responding. This causes a thrashing node to time out get-node-state requests, setting the node down in the cluster state.  Effect on cluster: This will have the same effects like the not available on network issue.    Constantly restarting distributor or service layer node  A broken node may end up with processes constantly restarting. It may die during initialization due to accessing corrupt files, or it may die when it starts receiving requests of a given type triggering a node local bug. This is bad for distributor nodes, as these restarts create constant ownership transfer between distributors, causing windows where buckets are unavailable.  The cluster controller has functionality for detecting such nodes. If a node restarts in a way that is not detected as a controlled shutdown, more than  max_premature_crashes, the cluster controller will set the wanted state of this node to be down.  Detecting a controlled restart is currently a bit tricky. A controlled restart is typically initiated by sending a TERM signal to the process. Not having any other sign, the content layer has to assume that all TERM signals are the cause of controlled shutdowns. Thus, if the process keep being killed by kernel due to using too much memory, this will look like controlled shutdowns to the content layer."}
    }
  },
  {
    "fields": {
      "path":      { "assign": "/documentation/reference/advanced-indexing-language.html"},
      "namespace": { "assign": "open"},
      "title":     { "assign": "Indexing Language"},
      "content":   { "assign": " This reference documents the full Vespa indexing language. If more complex processing of input data is required, implement a document processor.  The indexing language is analogous to UNIX pipes, in that statements consists of expressions separated by the pipe symbol where the output of each expression is the input of the next. Statements are terminated by semicolon and are independent from each other (except when using variables).     Indexing script  An indexing script is a sequence of indexing statements separated by a semicolon (;). A script is executed statement-by-statement, in order, one document at a time.  Vespa derives one indexing script per search cluster based on the search definitions assigned to that cluster. As a document is fed to a search cluster, it passes through the corresponding indexing cluster, which runs the document through its indexing script.  You can examine the indexing script generated for a specific search cluster by retrieving the configuration of the indexing document processor.  $ vespa-get-config -i search/cluster.&lt;cluster-name&gt; -n vespa.configdefinition.ilscripts  The current execution value is set to null prior to executing a statement.     Indexing statement  An indexing statement is a sequence of indexing expressions separated by a pipe (|). A statement is executed expression-by-expression, in order.  Within a statement, the execution value is passed from one expression to the next.    The simplest of statements passes the value of an input field into an attribute:  input year | attribute year;  The above statement consists of 2 expressions; input year and attribute year. The former sets the execution value to the value of the \"year\" field of the input document. The latter writes the current execution value into the attribute \"year\".     Indexing expression   Primitives  A string or numeric literal can be used as an expression to explicitly set the execution value (e.g. \"foo\" or 69).   Outputs  An output expression is an expression that writes the current execution value to a document field. These expressions also double as the indicator for the type of field to construct (i.e. attribute, index or summary). It is important to note that you can not assign different values to the same field in a single document (e.g. attribute | lowercase | index is illegal and will not deploy).             Expression      Description               attribute             Writes the execution value to the current field. During deployment,       this indicates that the field should be stored as an attribute.                 index             Writes the execution value to the current field. During deployment,       this indicates that the field should be stored as an index field.                 summary             Writes the execution value to the current field. During deployment,       this indicates that the field should be included in the document       summary.                Arithmetics  Indexing statements can contain any combination of arithmetic operations, as long as the operands are numeric values. In case you need to convert from string to numeric, or convert from one numeric type to another, use the applicable converter expression. The supported arithmetic operators are:             Operator      Description               &lt;lhs&gt; + &lt;rhs&gt;             Sets the execution value to the result of adding of the execution       value of the lhs expression with that of       the rhs expression.                 &lt;lhs&gt; - &lt;rhs&gt;             Sets the execution value to the result of subtracting of the execution       value of the lhs expression with that of       the rhs expression.                 &lt;lhs&gt; * &lt;rhs&gt;             Sets the execution value to the result of multiplying of the execution       value of the lhs expression with that of       the rhs expression.                 &lt;lhs&gt; / &lt;rhs&gt;             Sets the execution value to the result of dividing of the execution       value of the lhs expression with that of       the rhs expression.                 &lt;lhs&gt; % &lt;rhs&gt;             Sets the execution value to the remainder of dividing the execution       value of the lhs expression with that of       the rhs expression.                 &lt;lhs&gt; . &lt;rhs&gt;             Sets the execution value to the concatenation of the execution value       of the lhs expression with that of the rhs       expression. If both lhs and rhs are       collection types, this operator will append rhs       to lhs (if any operand is null, it is treated as an empty       collection). If not, this operator concatenates the string       representations of lhs and rhs (if any       operand is null, the result is null).                 You may use parenthesis to declare precedence of execution (e.g. (1   + 2) * 3). This also works for more advanced array concatenation   statements such as (input str_a | split ',') . (input str_b | split   ',') | index arr.     Converters  There are several expressions that allow you to convert from one data type to another. These are often used within a for_each to convert e.g. an array of strings to an array of integers.             Converter      Input      Output      Description               to_array      Any      Array&lt;inputType&gt;      Converts the execution value to a single-element array.            to_byte      Any      Byte             Converts the execution value to a byte. This will throw a       NumberFormatException if the string representation of the execution       value does not contain a parseable number.                 to_double      Any      Double             Converts the execution value to a double. This will throw a       NumberFormatException if the string representation of the execution       value does not contain a parseable number.                 to_float      Any      Float             Converts the execution value to a float. This will throw a       NumberFormatException if the string representation of the execution       value does not contain a parseable number.                 to_int      Any      Integer             Converts the execution value to an int. This will throw a       NumberFormatException if the string representation of the execution       value does not contain a parseable number.                 to_long      Any      Long             Converts the execution value to a long. This will throw a       NumberFormatException if the string representation of the execution       value does not contain a parseable number.                 to_pos      String      Position             Converts the execution value to a position struct. The input format       must be either a) [N|S]&lt;val&gt;;[E|W]&lt;val&gt;, or       b) x;y.                 to_string      Any      String      Converts the execution value to a string.            to_uri      String      Uri      Converts the execution value to a URI struct            to_wset      Any      WeightedSet&lt;inputType&gt;             Converts the execution value to a single-element weighted set with       default weight.                Other expressions  The following are the unclassified expressions available:             Expression Description               attribute &lt;fieldName&gt;      Writes the execution value to the named attribute field.            base64decode             If the execution value is a string, it is base-64 decoded to a long       integer. If it is not a string, the execution value is set       to Long.MIN_VALUE.                 base64encode             If the execution value is a long integer, it is base-64 encoded to a       string. If it is not a long integer, the execution value is set       to null.                 echo             Prints the execution value to standard output, for debug purposes.                 flatten             Sets the execution value to a new string which is the current value       with all linguistic annotations written into the string itself.       This is useful for testing various       tokenization settings on a field                 for_each { &lt;script&gt; }             Executes the given indexing script for each element in the execution       value. Here, element refers to each element in a collection, or each       field value in a struct.                 get_field &lt;fieldName&gt;             Retrieves the value of the named field from the execution value (which       needs to be either a document or a struct), and sets it as the new       execution value.                 get_var &lt;varName&gt;             Retrieves the value of the named variable from the execution context       and sets it as the execution value. Note that variables are scoped to       the indexing script of the current field.                 hex_decode             If the execution value is a string, it is parsed as a long integer in       base-16. If it is not a string, the execution value is set       to Long.MIN_VALUE.                 hex_encode             If the execution value is a long integer, it is converted to a string       representation of an unsigned integer in base-16. If it is not a long       integer, the execution value is set to null.                 hostname             Sets the execution value to the name of the host computer.                        if (&lt;lhs&gt; &lt;cmp&gt; &lt;rhs&gt;) {           &lt;trueScript&gt;       }       [ else { &lt;falseScript&gt; } ]                  Executes the trueScript if the conditional evaluates to       true, or the falseScript if it evaluates to false. If       either lhs or rhs is null, no expression is       executed.                 index &lt;fieldName&gt;      Writes the execution value to the named index field.            input &lt;fieldName&gt;             Retrieves the value of the named field from the document and sets it       as the execution value. The field name may contain '.' characters to       retrieve nested struct fields.                 join \"&lt;delim&gt;\"             Creates a single string by concatenating the string representation of       each array element of the execution value.       This function is useful or indexing data from a       multivalue field       into a singlevalue field.                 lowercase      Lowercases all the strings in the execution value.            ngram &lt;size&gt;             Adds ngram annotations to all strings in the execution value.                 normalize           normalize the input data.       The corresponding query command for this function is normalize.                 now             Outputs the current system clock time as a UNIX timestamp,       i.e. seconds since 0 hours, 0 minutes, 0 seconds, January 1, 1970,       Coordinated Universal Time (Epoch).                 random [ &lt;max&gt; ]             Returns a random integer value.       Lowest value is 0 and the highest value is determined either by the argument or,       if no argument is given, the execution value.                        select_input {        ( case &lt;fieldName&gt;: &lt;statement&gt;; )*        }                  Performs the statement that corresponds to the first named       field that is not empty (see example).                 set_language             Sets the language of this document to the string representation of the execution value.       Parses the input value as an RFC 3066 language tag,       and sets that language for the current document.       This affects the behavior of the tokenizer.       The recommended use is to have one field in the document containing the language code,       and that field should be the first field in the document,       as it will only affect the fields defined after it in the schema.       Read linguistics       for more information on how language settings are applied.                 set_var &lt;varName&gt;             Writes the execution value to the named variable. Note that variables       are scoped to the indexing script of the current field.                 substring &lt;from&gt; &lt;to&gt;             Replaces all strings in the execution value by a substring of the respective value.       The arguments are inclusive-from and exclusive-to.       Both arguments are clamped during execution to avoid going out of bounds.                 split &lt;regex&gt;             Splits the string representation of the execution value into a string       array using the given regex pattern.       This function is useful for creating       multivalue       fields such as an integer array out of a string       of comma-separated numbers.                 summary &lt;fieldName&gt;             Writes the execution value to the named summary field.       Summary fields of type string are limited to 64kB.        If a larger string is stored, the indexer will issue a warning and       truncate the value to 64kB.                        switch {        ( case '&lt;value&gt;': &lt;caseStatement&gt;; )*        [ default: &lt;defaultStatement&gt;; ]        }                  Performs the statement of the case whose value matches the string       representation of the execution value (see       example).                 tokenize [ normalize ] [ stem ]             Adds linguistic annotations to all strings in the execution value. Read linguistics       for more information.                  trim             Removes leading and trailing whitespace from all strings in the       execution value.                 uri             Converts all strings in the execution value to an URI struct. If a       string could not be converted, it is removed.                 Select_input example  The select_input expression is used to choose a statement to execute based on which fields are non-empty in the input document:   select_input {   CX:   input CX | set_var CX;   CA:   input CA . \" \" . input CB | set_var CX; }   This statement executes input CX | set_var CX; unless CX is empty. If so, it will execute input CA . \" \" . input CB | set_var CX; unless CA is empty.     Switch example  The switch-expression behaves similarly to the switch-statement in other programming languages. Each case in the switch-expression consists of a string and a statement. The execution value is compared the each string, and if there is a match, the corresponding statement is executed. An optional default operation (designated by default:) can be added to the end of the switch:   input mt | switch {   case \"audio\": input fa | index;   case \"video\": input fv | index;   default: 0 | index; };     Indexing statements example  Using indexing statements, multiple document fields can be used to produce one index structure field. For example, the index statement:  input field1 . input field2 | attribute field2;  combines field1 and field2 into the attribute named field2. When partially updating documents which contains indexing statement which combines multiple fields the following rules apply:    Only attributes where all the source values are available in     the source document update will be updated   The document update will fail when indexed (only) if no     attributes end up being updated when applying the rule above   Example: If a schema has the indexing statements  input field1 | attribute field1; input field1 . input field2 | attribute field2;  the following will happen for the different partial updates:         Partial update contains Result       field1 field1 is updated    field2 The update fails    field1 and field2 field1 and field2 are updated"}
    }
  },
  {
    "fields": {
      "path":      { "assign": "/documentation/annotations.html"},
      "namespace": { "assign": "open"},
      "title":     { "assign": "Annotations API"},
      "content":   { "assign": " This document describes the Vespa Annotations API; its purpose and use cases along with some usage examples.     Annotating Text  Using Annotations as Simple Labels  Imagine a use case where one wants to add some metadata to a chunk of text, where various parts of the text have some semantics that we want to express.              Fig. 0. A block of text.       This can be done by marking up the text with spans—where a span is identified by a start character index, and a length, and grouping these spans together to form a span tree.              Fig. 1. A block of text with spans over it.       In the illustration above, we have a span tree called “html”, with a root node that holds references to the spans we have created over the text.  To do this using the Annotations API, use the following code:    StringFieldValue text = new StringFieldValue(\"&lt;html&gt;&lt;head&gt;&lt;title&gt;Diary&lt;/title&gt;&lt;/head&gt;&lt;body&gt;I live in San Francisco&lt;/body&gt;&lt;/html&gt;\");  SpanList root = new SpanList(); root.add(new Span(0, 19))         .add(new Span(19, 5))         .add(new Span(24, 21))         .add(new Span(45, 23))         .add(new Span(68, 14));  SpanTree tree = new SpanTree(\"html\", root); text.setSpanTree(tree);    Now for each of the spans over the text, we can add an arbitrary number of annotations. An annotation is a piece of information associated with a span. For now, think of it as a label.              Fig. 2. Each span has an annotation on it.       Annotations are kept by the span tree in a global list. The annotations in the list have references to their respective spans. To do this using the Annotations API, first declare the annotation types in your schema:    schema example {    annotation text {   }   annotation markup {   }  }    Then, use the declared types and annotate the spans:    // The following line works inside process(Processing) in a DocumentProcessor AnnotationTypeRegistry atr = processing.getService().getDocumentTypeManager().getAnnotationTypeRegistry();  StringFieldValue text = new StringFieldValue(\"&lt;html&gt;&lt;head&gt;&lt;title&gt;Diary&lt;/title&gt;&lt;/head&gt;&lt;body&gt;I live in San Francisco&lt;/body&gt;&lt;/html&gt;\");  AnnotationType textType = atr.getType(\"text\"); AnnotationType markup = atr.getType(\"markup\");  SpanList root = new SpanList(); SpanTree tree = new SpanTree(\"html\", root);   Span span1 = new Span(0, 19); root.add(span1); tree.annotate(span1, markup);  Span span2 = new Span(19, 5); root.add(span2); tree.annotate(span2, textType);  Span span3 = new Span(24, 21); root.add(span3); tree.annotate(span3, markup);  Span span4 = new Span(45, 23); root.add(span4); tree.annotate(span4, textType);  Span span5 = new Span(68, 14); root.add(span5); tree.annotate(span5, markup);   text.setSpanTree(tree);    Note that in the above code, we have used a convenience method SpanTree.annotate(SpanNode node, AnnotationType at). This is equivalent to:    AnnotationType markupType = new AnnotationType(\"markup\"); Annotation a = new Annotation(markupType); tree.annotate(span, a);    Annotation Trees  The annotated spans shown in Fig. 2 might be fine for the very simple cases where one wants to just annotate some text. However, let's imagine that one wants to not only identify markup from text, but also create a structure over the markup.    In such a case, we can build a tree of spans using SpanLists. A SpanList is simply a node in the tree that can have children—the children can be spans, or SpanLists themselves. And of course, SpanLists can be annotated as well. Henceforth we will refer to both Spans and SpanLists as SpanNodes, which is in fact their common superclass.              Fig. 3. An annotation tree.       In Fig. 3, we no longer have a simple two-level structure of Spans with labels on them, but instead a tree of SpanNodes, each having zero or more annotations.    To do this using the Annotations API, first declare the annotation types in your schema:    schema example {    annotation text {   }   annotation begintag {   }   annotation endtag {   }   annotation body {   }   annotation header {   }  }    Then, use the declared types and annotate the spans:    // The following line works inside process(Processing) in a DocumentProcessor AnnotationTypeRegistry atr = processing.getService().getDocumentTypeManager().getAnnotationTypeRegistry();  StringFieldValue text = new StringFieldValue(\"&lt;html&gt;&lt;head&gt;&lt;title&gt;Diary&lt;/title&gt;&lt;/head&gt;&lt;body&gt;I live in San Francisco&lt;/body&gt;&lt;/html&gt;\");  SpanList root = new SpanList(); SpanTree tree = new SpanTree(\"html\", root);  AnnotationType textType = atr.getType(\"text\"); AnnotationType beginTag = atr.getType(\"begintag\"); AnnotationType endTag = atr.getType(\"endtag\"); AnnotationType bodyType = atr.getType(\"body\"); AnnotationType headerType = atr.getType(\"header\");  SpanList header = new SpanList(); {     Span span1 = new Span(6, 6);     Span span2 = new Span(12, 7);     Span span3 = new Span(19, 5);     Span span4 = new Span(24, 8);     Span span5 = new Span(32, 7);     header.add(span1)             .add(span2)             .add(span3)             .add(span4)             .add(span5);     tree.annotate(span1, beginTag)             .annotate(span2, beginTag)             .annotate(span3, textType)             .annotate(span4, endTag)             .annotate(span5, endTag)             .annotate(header, headerType); }  SpanList body = new SpanList(); {     Span span1 = new Span(39, 6);     Span span2 = new Span(45, 23);     Span span3 = new Span(68, 7);     body.add(span1)             .add(span2)             .add(span3);     tree.annotate(span1, beginTag)             .annotate(span2, textType)             .annotate(span3, endTag)             .annotate(body, bodyType); }  {     Span span1 = new Span(0, 6);     Span span2 = new Span(75, 7);     root.add(span1)             .add(header)             .add(body)             .add(span2);     tree.annotate(span1, beginTag)             .annotate(span2, endTag); }  text.setSpanTree(tree);    Annotation values  But what if we need to attach more information to a SpanNode than just a label?  Imagine that we want to annotate “San Francisco” in the text above with not only “city”, but also include its latitude and longitude. This can be done, since annotations can also have values.    Every annotation in the tree is of a declared annotation type, where an annotation type is declared with a name and a possible data type for its optional value. Up until now, our annotation types have only had names, and no data type.    For the case of “San Francisco”, we can let our annotation type have two data fields, as shown in the schema syntax below:    schema example {    annotation text {   }   annotation begintag {   }   annotation endtag {   }   annotation body {   }   annotation header {   }   annotation city {     field latitude type double {}     field longitude type double {}   }  }    By deploying the schema above, a struct data type is implicitly created, named annotation.city, having the two fields declared. The annotation type city is set to use this data type. For more on struct types, see the schema reference.    We can then create an annotation holding the latitude and longitude of San Francisco on this SpanNode.              Fig. 4. One annotation has a value.       To do this using the Annotations API, use the following code:    //the following line works inside process(Processing) in a DocumentProcessor AnnotationTypeRegistry atr = processing.getService().getDocumentTypeManager().getAnnotationTypeRegistry();  StringFieldValue text = new StringFieldValue(\"&lt;html&gt;&lt;head&gt;&lt;title&gt;Diary&lt;/title&gt;&lt;/head&gt;&lt;body&gt;I live in San Francisco&lt;/body&gt;&lt;/html&gt;\");  SpanList root = new SpanList(); SpanTree tree = new SpanTree(\"html\", root);  AnnotationType textType = atr.getType(\"text\"); AnnotationType beginTag = atr.getType(\"begintag\"); AnnotationType endTag = atr.getType(\"endtag\"); AnnotationType bodyType = atr.getType(\"body\"); AnnotationType headerType = atr.getType(\"header\"); AnnotationType cityType = atr.getType(\"city\");  Struct position = (Struct) cityType.getDataType().createFieldValue(); position.setValue(\"latitude\", 37.774929); position.setValue(\"longitude\", -122.419415); Annotation city = new Annotation(cityType, position);  SpanList header = new SpanList(); {     Span span1 = new Span(6, 6);     Span span2 = new Span(12, 7);     Span span3 = new Span(19, 5);     Span span4 = new Span(24, 8);     Span span5 = new Span(32, 7);     header.add(span1)             .add(span2)             .add(span3)             .add(span4)             .add(span5);     tree.annotate(span1, beginTag)             .annotate(span2, beginTag)             .annotate(span3, textType)             .annotate(span4, endTag)             .annotate(span4, endTag)             .annotate(header, headerType); }  SpanList textNode = new SpanList(); {     Span span1 = new Span(45, 10);     Span span2 = new Span(55, 13);     textNode.add(span1)             .add(span2);     tree.annotate(span2, city)             .annotate(textNode, textType); }  SpanList body = new SpanList(); {     Span span1 = new Span(39, 6);     Span span2 = new Span(68, 7);     body.add(span1)             .add(textNode)             .add(span2);     tree.annotate(span1, beginTag)             .annotate(span2, endTag)             .annotate(body, bodyType); }  {     Span span1 = new Span(0, 6);     Span span2 = new Span(75, 7);     root.add(span1)             .add(header)             .add(body)             .add(span2);     tree.annotate(span1, beginTag)             .annotate(span2, endTag); }  text.setSpanTree(tree);    Alternate subtrees  For the examples above, the purpose of the annotator has been to express the structure of the original HTML document, as well as adding some semantics to the tree. The HTML structure is fairly unambiguous (let's assume valid HTML for now). However, there are many other use cases where the source text allows for multiple interpretations, i.e. where there is not one unambiguous tree. Natural language processing is one such use case.    As an example, review the following sentence:    I saw the girl with the boy.    Most humans would read this as \"the boy is accompanying the girl, and I saw them both\". There is one alternate interpretation; that \"boy\" is an instrument that could be used to see the girl, as in \"I saw the girl with the telescope\", i.e. \"I saw the girl using the telescope\". NLP parsers would likely identify both these interpretations.    We can express more than one interpretation in one span tree, using an AlternateSpanList. As opposed to a SingleSpanList, which can have a single subtree of SpanNodes, AlternateSpanList can have an arbitrary number of subtrees, each with its own probability. In the analysis of longer and more complex passages of text, this is a great advantage, as we don't have to copy the entire tree to express differing interpretations. We just insert an AlternateSpanList at the point in the tree where the interpretations differ, and attach suitable probabilities to them, if possible.    Annotation References  Annotations can in fact have references to other annotations in the tree, that is, have an Annotation reference as its value.    Review the example given in Fig. 6 below:              Fig. 6. HTML structure where San and Francisco do not have common supernode.       We can see that in the HTML structure, “I live in San” is one paragraph, while “Francisco” continues on the next line. Consequently, “San” and “Francisco” do not have a SpanList as their immediate common supernode. On a higher semantic level, though, it is clear that “San Francisco” should be annotated as a city, as in Fig. 4. This can be achieved by using an annotation reference:              Fig. 7. HTML structure with annotation reference.       Note that the annotation “city” is not annotating a span node. It is present in the global list of annotations, and has references to other annotations in the same list. To create the structure as shown in Fig. 7 above, we must declare the struct position, and change the fields of annotation type city.     schema example {    annotation text {   }   annotation begintag {   }   annotation endtag {   }   annotation body {   }   annotation header {   }   annotation city {     field position type position {}     field references type array&lt;annotationref&lt;text&gt;&gt; {}   }   struct position {     field latitude type double {}     field longitude type double {}   }  }    To do this using the Annotations API, use the following code:    //the following two lines work inside process(Processing) in a DocumentProcessor DocumentTypeManager dtm = processing.getService().getDocumentTypeManager(); AnnotationTypeRegistry atr = dtm.getAnnotationTypeRegistry();  StringFieldValue text = new StringFieldValue(\"&lt;body&gt;&lt;p&gt;I live in San &lt;/p&gt;Francisco&lt;/body&gt;\");  SpanList root = new SpanList(); SpanTree tree = new SpanTree(\"html\", root);  StructDataType positionType = (StructDataType) dtm.getDataType(\"position\");  AnnotationType textType = atr.getType(\"text\"); AnnotationType beginTag = atr.getType(\"begintag\"); AnnotationType endTag = atr.getType(\"endtag\"); AnnotationType bodyType = atr.getType(\"body\"); AnnotationType paragraphType = atr.getType(\"paragraph\"); AnnotationType cityType = atr.getType(\"city\");  Struct position = new Struct(positionType); position.setValue(\"latitude\", 37.774929); position.setValue(\"longitude\", -122.419415);  Annotation sanAnnotation = new Annotation(textType); Annotation franciscoAnnotation = new Annotation(textType);  Struct positionWithRef = (Struct) cityType.getDataType().createFieldValue(); positionWithRef.setValue(\"position\", position);  Field referencesField = ((StructDataType) cityType.getDataType()).getField(\"references\"); Array&lt;FieldValue&gt; refList = new Array&lt;FieldValue&gt;(referencesField.getDataType()); AnnotationReferenceDataType annRefType = (AnnotationReferenceDataType) ((ArrayDataType) referencesField.getDataType()).getNestedType(); refList.add(new AnnotationReference(annRefType, sanAnnotation)); refList.add(new AnnotationReference(annRefType, franciscoAnnotation)); positionWithRef.set(referencesField, refList);  Annotation city = new Annotation(cityType, positionWithRef);  SpanList paragraph = new SpanList(); {     Span span1 = new Span(6, 3);     Span span2 = new Span(9, 10);     Span span3 = new Span(19, 4);     Span span4 = new Span(23, 4);     paragraph.add(span1)             .add(span2)             .add(span3)             .add(span4);     tree.annotate(span1, beginTag)             .annotate(span2, textType)             .annotate(span3, sanAnnotation)             .annotate(span4, endTag)             .annotate(paragraph, paragraphType); }  {     Span span1 = new Span(0, 6);     Span span2 = new Span(27, 9);     Span span3 = new Span(36, 8);     root.add(span1)             .add(paragraph)             .add(span2)             .add(span3);      tree.annotate(span1, beginTag)             .annotate(span2, franciscoAnnotation)             .annotate(span3, endTag)             .annotate(root, bodyType)             .annotate(city); }  text.setSpanTree(tree);    The above example shows that when using annotation references, building the span tree, and overlaying annotations (which now form an annotation graph), becomes quite complex. However, it enables annotators from various contexts to cooperate on one single annotation graph.    In the above example, we are mixing two semantically different trees into one tree. The first tree models the HTML representation of the input document. The second tree tries to find entities (like “San Francisco”), and creates a structure on a higher semantic level.    Note that in some cases, it would be wiser to create two span trees, and annotating these separately. Recall that on the last line in all the above code samples, we have set the tree on the StringFieldValue using StringFieldValue.setSpanTree(String s, SpanNode sn). The string given is an arbitrary name for this tree. Creating two trees is then trivial (and is left as an exercise to the reader).     Manipulating a Span Tree  The previous section focused mainly on building a span tree over an input string. In many cases though, like when using the docproc framework, a document processor reads a span tree created by some previous process, manipulates it, and passes it on.    Iterating over SpanNodes  A typical use case is to iterate over all SpanNodes (that have an Annotation of a certain type), and manipulate these. As an example, imagine that Fig. 2 above is the output of one document processor and Fig. 3 is the output of another. The second document processor would typically iterate over all nodes that have an annotation of type “markup”, and replace them with spans that have annotations of type “begintag” and “endtag”.              Fig. 8. Traversing nodes that have annotations of type “markup” in a span tree.       To do this using the Annotations API, use the following code:    public void example() {     StringFieldValue text = new StringFieldValue(\"&lt;html&gt;&lt;head&gt;&lt;title&gt;Diary&lt;/title&gt;&lt;/head&gt;&lt;body&gt;I live in San Francisco&lt;/body&gt;&lt;/html&gt;\");      SpanTree tree = text.getSpanTree(\"html\");     SpanList root = (SpanList) tree.getRoot();     //TODO: Note that the above could have been a Span or an AlternateSpanList!      ListIterator&lt;SpanNode&gt; nodeIt = root.childIterator();      AnnotationType beginTag = new AnnotationType(\"begintag\");     AnnotationType endTag = new AnnotationType(\"endtag\");       while (nodeIt.hasNext()) {         SpanNode node = nodeIt.next();         boolean nodeHadMarkupAnnotation = removeMarkupAnnotation(tree, node);         if (nodeHadMarkupAnnotation) {             nodeIt.remove();             List&lt;Span&gt; replacementNodes = analyzeMarkup(tree, node, text, beginTag, endTag);             for (SpanNode repl : replacementNodes) {                 nodeIt.add(repl);             }         }     } }  /**  * Removes annotations of type 'markup' from the given node.  *  * @param tree the tree to remove annotations from  * @param node the node to remove annotations of type 'markup' from  * @return true if the given node had 'markup' annotations, false otherwise  */ private boolean removeMarkupAnnotation(SpanTree tree, SpanNode node) {     //get iterator over all annotations on this node:     Iterator&lt;Annotation&gt; annotationIt = tree.iterator(node);      while (annotationIt.hasNext()) {         Annotation annotation = annotationIt.next();         if (annotation.getType().getName().equals(\"markup\")) {             //this node has an annotation of type markup, remove it:             annotationIt.remove();             //return true, this node had a markup annotation:             return true;         }     }     //this node did not have a markup annotation:     return false; }  /**  * NOTE: This method is provided only for completeness. It analyzes spans annotated with  * \"markup\", and splits them into several shorter spans annotated with \"begintag\"  * and \"endtag\".  *  * @param tree       the span tree to annotate into  * @param input      a SpanNode that is annotated with \"markup\".  * @param text       the text that the SpanNode covers  * @param beginTag   the type to use for begintag annotations  * @param endTagType the type to use for endtag annotations  * @return a list of new spans to replace the input  */ private List&lt;Span&gt; analyzeMarkup(SpanTree tree, SpanNode input, StringFieldValue text,                                  AnnotationType beginTag, AnnotationType endTagType) {     //we know that this node is annotated with \"markup\"     String coveredText = input.getText(text.getString()).toString();     int spanOffset = input.getFrom();     int tagStart = -1;     boolean endTag = false;     List&lt;Span&gt; tags = new ArrayList&lt;Span&gt;();     for (int i = 0; i &gt; coveredText.length(); i++) {         if (coveredText.charAt(i) == '&lt;') {             //we're in a tag             tagStart = i;             continue;         }         if (coveredText.charAt(i) == '&gt;' &amp;&amp; tagStart &gt; -1) {             Span span = new Span(spanOffset + tagStart, (i + 1) - tagStart);             tags.add(span);             if (endTag) {                 tree.annotate(span, endTagType);             } else {                 tree.annotate(span, beginTag);             }             tagStart = -1;         }         if (tagStart &gt; -1 &amp;&amp; i == (tagStart + 1)) {             if (coveredText.charAt(i) == '/') {                 endTag = true;             } else {                 endTag = false;             }         }     }     return tags; }    Iterating over annotations  One may also traverse the global list of annotations, as opposed to iterating over SpanNodes.  Imagine a use case where some annotator wants to find and remove all annotations of type “markup”.              Fig. 9. Traversing the tree and iterating over annotations of type \"markup\", and then deleting them.       To do this using the Annotations API, use the following code:    StringFieldValue text = new StringFieldValue(\"&lt;html&gt;&lt;head&gt;&lt;title&gt;Diary&lt;/title&gt;&lt;/head&gt;&lt;body&gt;I live in San Francisco&lt;/body&gt;&lt;/html&gt;\");   SpanTree tree = text.getSpanTree(\"html\");  ListIterator&lt;Annotation&gt; annotationIt = tree.iterator();  while (annotationIt.hasNext()) {     Annotation annotation = annotationIt.next();     if (annotation.getType().getName().equals(\"markup\")) {         //we have an annotation of type markup, remove it:         annotationIt.remove();     } }     Annotation Inheritance  Annotation types can inherit from each other. This is particularly useful when given e.g. a document processor (along with its configuration of annotation types and document types) from some external entity, and one wants to extend these annotation types with some additional information.  Review the below example:    schema example {    annotation person {     field birthdate type int { }     field firstname type string { }     field lastname type string { }   }  }    This annotation type, person, comes from some legacy code that we have gotten from some external entity. We want to leave this code and this configuration as-is, but we are writing document processors that rely on these types and extend them:    search example2 {    annotation employee inherits person {     field employeeid type int { }   }  }    The type employee behaves just like a person, and can be used anywhere that a person can appear.  It has inherited the three fields defined in person, and has one field of its own in addition."},
      "outlinks":  { "assign": [
        "/documentation/operations/admin-procedures.html"
      ]}
    }
  },
  {
    "fields": {
      "path":      { "assign": "/documentation/content/api-state-rest-api.html"},
      "namespace": { "assign": "open"},
      "title":     { "assign": "State API"},
      "content":   { "assign": "     The cluster controller has a RESTful API for viewing and modifying     the state of a content cluster.     To find the URL to access the State API, identify the cluster controller services in your application.     Only the master cluster controller will be able to respond.     The master cluster controller is the cluster controller alive that has the     lowest index. Thus, you will typically use cluster controller 0, but if you fail     to contact it, you need to try number 1 and so on.     Using the vespa-model-inspect command line tool:   $ vespa-model-inspect service -u container-clustercontroller container-clustercontroller @ hostname.domain.com : admin admin/cluster-controllers/0     http://hostname.domain.com:19050/ (STATE EXTERNAL QUERY HTTP)     http://hostname.domain.com:19117/ (EXTERNAL HTTP)     tcp/hostname.domain.com:19118 (MESSAGING RPC)     tcp/hostname.domain.com:19119 (ADMIN RPC)       In this example, there is only one clustercontroller, and the State Rest API is     available on the port marked STATE and HTTP, namely 19050 in this example. This     information can also be retrieved through the model config in the config server.   Types        Type      Spec      Example      Description         cluster      &lt;identifier&gt;      music               The name given to a content cluster in a Vespa application.             description      .*      Some \\\"JSON escaped\\\" text               Description can contain anything that is valid JSON. However, as the         information is presented in various interfaces, some which may present reasons         for all the states in a cluster or similar, keeping it short and to the         point makes it easier to fit the information neatly into a table and get a         better cluster overview.             group-spec      &lt;identifier&gt;(\\.&lt;identifier&gt;)*      asia.switch0               The hierarchical group assignment of a given content node. This is a         dot separated list of identifiers given in the application services.xml         configuration.             node      [0-9]+      0               The index or distribution key identifying a given node within the         context of a content cluster and a service type.             partition      [0-9]+      0               The index of a partition within the context of a content cluster, a         service type and a node.             service-type      (distributor|storage)      distributor               The type of the service to look at state for, within the context of a given         content cluster.             state-disk      (up|down)      up               One of the valid disk states.             state-generated      (initializing|up|down|retired|maintenance)      up               One of the valid node generated states.             state-unit      (initializing|up|stopping|down)      up               One of the valid node unit states.             state-user      (up|down|retired|maintenance)      up               One of the valid node user states.           Errors      Errors will be indicated using the HTTP status codes. An error response from the     State API will include a JSON encoded error response with extra information. As a     request may fail outside of the State API, we can not guarantee that such a JSON     representation exist though. To make it simpler for clients, all errors they need     to handle specifically should be specified in HTTP error codes. Thus the content     of the JSON error report, if it exist, can be left unspecified, and just used to     improve an error report if needed.     Note: Do not depend on the JSON content for anything other than improving     error reports - contents may change at any time   Cluster controller not master - master known      This error means you are talking to the wrong cluster controller. This will give     you a standard HTTP redirect, so your HTTP client may automatically redo the     request on the correct cluster controller. If it does not you might want to handle     this specifically.       Note that since we know the cluster controller available with the     lowest index will be the master, you will typically try to query the     cluster controllers in index order, in which case you are unlikely to ever     get this error, but rather fail to connect to the cluster controller if it     is not the current master.        HTTP/1.1 303 See Other     Location: http://&lt;master&gt;/&lt;current-request&gt;     Content-Type: application/json      {         \"message\" : \"Cluster controller index not master. Use master at index index.     }   Cluster controller not master - unknown or no master      This error is used if the cluster controller asked is not master, and     it doesn't know who the master is. This can happen, for instance in a network     split where cluster controller 0 no longer can reach cluster controller 1 and 2,     in which case cluster controller 0 knows it is not master as it can't see the     majority, and cluster controller 1 and 2 will vote 1 to master.       HTTP/1.1 503 Service Unavailable     Content-Type: application/json      {         \"message\" : \"No known master cluster controller currently exist.\"     }    Recursive mode      To use recursive mode, specify the recursive URL parameter, and     give it a numeric value for number of levels.     A value of true is also valid, this returns all levels.     Examples: Use recursive=1 for a node request     to also see all the partition data, use recursive=2     to see all the node data within each     service type, without getting all the partition information too.       In recursive mode, you will see the same output as you find in the spec below.     However, where there is a { \"link\" : \"&lt;url-path&gt;\" } element, this     element will be replaced by the content of that request, given a recursive value     of one less than the request above.   Functions      Here follows a list of all the available functions. Note that more headers than     the ones specified will exist. All requests with content will obviously have a     content length for instance.   List the existing content clusters      HTTP GET /cluster/v2  Example success result:      HTTP/1.1 200 OK     Content-Type: application/json      {         \"cluster\" : {             \"music\" : {                 \"link\" : \"/cluster/v2/music\"             },             \"books\" : {                 \"link\" : \"/cluster/v2/books\"             }         }     }   Get cluster state and list the various service types within the cluster      HTTP GET /cluster/v2/&lt;cluster&gt;  Example success result:      HTTP/1.1 200 OK     Content-Type: application/json      {         \"state\" : {             \"generated\" : {                 \"state\" : \"&lt;state-generated&gt;\",                 \"reason\" : \"&lt;description&gt;\"             }         }         \"service\" : {             \"distributor\" : {                 \"link\" : \"/cluster/v2/mycluster/distributor\"             },             \"storage\" : {                 \"link\" : \"/cluster/v2/mycluster/storage\"             }         {     }   List the nodes of a given service type for a cluster      HTTP GET /cluster/v2/&lt;cluster&gt;/&lt;service-type&gt;  Example success result:      HTTP/1.1 200 OK     Content-Type: application/json      {         \"node\" : {             \"0\" : {                 \"link\" : \"/cluster/v2/mycluster/storage/0\"             },             \"1\" : {                 \"link\" : \"/cluster/v2/mycluster/storage/1\"             }         }     }   Get node state      HTTP GET /cluster/v2/&lt;cluster&gt;/&lt;service-type&gt;/&lt;node&gt;  Example success result:      HTTP/1.1 200 OK     Content-Type: application/json      {         \"attributes\" : {             \"hierarchical-group\" : \"&lt;group-spec&gt;\"         },         \"partition\" : {             \"0\" : {                 \"link\" : \"/cluster/v2/mycluster/storage/0/0\"             }         },         \"state\" : {             \"unit\" : {                 \"state\" : \"&lt;state-unit&gt;\",                 \"reason\" : \"&lt;description&gt;\"             },             \"generated\" : {                 \"state\" : \"&lt;state-generated&gt;\",                 \"reason\" : \"&lt;description&gt;\"             },             \"user\" : {                 \"state\" : \"&lt;state-user&gt;\",                 \"reason\" : \"&lt;description&gt;\"             }         }     }   Get partition state      HTTP GET /cluster/v2/&lt;cluster&gt;/&lt;service-type&gt;/&lt;node&gt;/&lt;partition&gt;  Example success result:      HTTP/1.1 200 OK     Content-Type: application/json      {         \"metrics\" : {             \"bucket-count\" : &lt;integer&gt;,             \"unique-document-count\" : &lt;integer&gt;,             \"unique-document-total-size\" : &lt;integer&gt;          },         \"state\" : {             \"generated\" : {                 \"state\" : \"&lt;state-disk&gt;\",                 \"reason\" : \"&lt;description&gt;\"             },         }     }   Set node user state      HTTP PUT /cluster/v2/&lt;cluster&gt;/&lt;service-type&gt;/&lt;node&gt;     Content-Type: application/json      {         \"state\" : {             \"user\" : {                 \"state\" : \"retired\",                 \"reason\" : \"This colo will be removed soon\"             }         }     }  Success result:      HTTP/1.1 200 OK     Content-Type: application/json          {         \"wasModified\": true,         \"reason\": \"ok\"     }    "},
      "outlinks":  { "assign": [
        "/documentation/access-logging.html",
        "/documentation/operations/admin-procedures.html"
      ]}
    }
  }
]
